- Page for the #ai-workshop channel on Discord
- We read a paper/article or do a small learning task every week. New readings and prompts are posted on Tuesdays.
- Week 1: [ML Perf paper](https://arxiv.org/pdf/1910.01500.pdf)
    - Venkat: ML Perf is more like SATs or GRE than like traditional hardware benchmarking, which is more like EEGs or fMRI
- Week 2: [Taxonomy of NNs](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)
    - Prompt: What would good 2x2 axes for this taxonomy be?
- Week 3: Transformers, read at least one of the 3 things below
    - Good [less-technical explainer](https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication)
    - Good [more-technical explainer](https://jalammar.github.io/illustrated-transformer/)
    - The original paper, [Attention is all you need](https://arxiv.org/abs/1706.03762) is really hard...
- Week 4: Try out this [Browser-based tensorflow](https://playground.tensorflow.org/) for 15 minutes and share a screenshot
- Week 5: Spend 15 minutes trying out this browser-based model training wizard to recognize a voice phrase or object (via Anuraj) 
- Week 6: Read and comment on this article from 2021, focusing on insights that are still fresh versus things that have become obsolete [AI is learning how to create itself](https://www.technologyreview.com/2021/05/27/1025453/artificial-intelligence-learning-create-itself-agi/)
- Week 7: Read up on the basics of discrete convolutions and make up and solve by hand a simple example of a 2d convolution on a 5x5 black and white "image" matrix of 0s and 1 and a 3x3 "convolution mask." Either with code or pencil and paper, generate before/after images (use partial hatching to indicate grayscale). There are many tutorials. Here is a decent one, but if you find a better one, post a link here. Note that the Wikipedia and Wolfram Mathworld pages are much more technical and cover continuous domain so don't start there unless you are already comfortable with the math. [2d convolution tutorial](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)
- Week 8: Comment on this article on AI art by Holly Herndon and Mat Dryhurst: https://mirror.xyz/herndondryhurst.eth/eZG6mucl9fqU897XvJs0vUUMnm5OITpSWN8S-6KWamY
- Week 9: First read Harry Frankfurt's classic article On Bullshit http://www2.csudh.edu/ccauthen/576f12/frankfurt__harry_-_on_bullshit.pdf, then read Robin Sloan's article Notes on a Genre https://www.robinsloan.com/lab/notes-on-a-genre/ and answer the question: Is AI a bullshit engine?
- 
- Backlog/hopper
    - https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/
    - Hands on 1
        - Google Colab
    - Hands on 2
        - For technical people: Install PyTorch (Python/Anaconda first if necessary)
        - For non-technical people: Find and use an AI-based image-manipulation tool
    - Latent spaces https://www.nature.com/articles/s42256-020-0164-7
    - Software 2.0 https://www.youtube.com/watch?v=y57wwucbXR8
    - Dall-e 2  and PaLM video https://www.youtube.com/watch?v=RJwPN4qNi_Y
    - TVM https://tvm.apache.org/
    - Response:
- I was most impressed by the mapping of position to the sinusoidal space - and meditated on mapping (visual ml is as move from high dimension/image to low dimension/numbers, but this example is a move from low dimension to high dimension, position and layer ) https://medium.com/@hackerm0m/ursula-le-guin-and-machine-learning-303e4e327b73
